import asyncio
import base64
import functools
import io
import json
import random
import traceback
import uuid
from typing import Optional

import torch
from fastapi import APIRouter, Depends
from openai import OpenAI
from PIL import Image
from sqlalchemy.orm import Session
from sse_starlette import EventSourceResponse

from augment.measure import cnn_measure, openai_measure
from augment.req_and_resp import *
from augment.sd_pipeline import StableDiffusionUnified, reserved_lora_modules
from augment.simple_gan.model import Generator as Model
from base import create_response
from base.file_delegate import get_operator, s3_properties
from base.logger import logger
from base.nacos_config import get_db
from db.tool_model.tool_model_crud import get_tool_model
from label.client import get_model
from label.tools import base64_to_pil_image, pil_to_base64

model_path = "generator.pth"

model = Model(z_dim=2048, img_channels=3).to("cpu")
model.load_state_dict(torch.load(model_path))


router = APIRouter(
    prefix="/augment",
    tags=["Augment"],
)

__RESERVED_NEGATIVE_PROMPT__ = "cartoon, drawing, 3d, low quality, low resolution, blurry, unrealistic defect, overexposed, jagged edges, obvious collage, distorted geometry"


# TODO merge to augment, just for demo
@router.post("/gan/generate/stream")
async def gan_generate_stream(req: GANRequest):
    """Stream-generated images from the GAN model"""

    async def image_generator():
        operator = get_operator(s3_properties.augment_bucket_name)
        with torch.no_grad():
            z = torch.randn(req.count, 2048).to("cpu")
            generated_image = model(z)
            generated_image = (generated_image * 0.5) + 0.5
            # print(f"shape. {generated_image.shape}" )
            for img_tensor in generated_image:
                img_tensor = (
                    img_tensor.permute(1, 2, 0).clamp(0, 1).mul(255).byte().numpy()
                )
                pil_img = Image.fromarray(img_tensor)
                buf = io.BytesIO()
                pil_img.save(buf, format="PNG")
                buf.seek(0)
                img_name = str(uuid.uuid4()) + ".png"
                img_bytes = buf.getvalue()
                operator.write(img_name, img_bytes)
                yield f"path: {img_name}\n"
                await asyncio.sleep(0.5)

        # yield "[DONE]"

    return EventSourceResponse(
        image_generator(),
        media_type="text/event-stream",
    )


@router.post("/cv/generate/stream")
async def cv_generate_stream(req: CvAugmentRequest):
    """Stream-generated images from the cv model"""
    from mltools.augmentation.aug_no_label import random_aug_stream
    from mltools.utils.json2mask.third_party import img_b64_to_arr

    if req.types is None or len(req.types) == 0:
        logger.info("[cv augmentation] no types provided, using default: [rotation]")
        req.types = [
            "rotation",
        ]

    async def image_generator():
        operator = get_operator(s3_properties.augment_bucket_name)
        img = img_b64_to_arr(req.b64)

        for aug_img in random_aug_stream(
            img,
            augNumber=req.count,
            augMethods=req.types,
        ):
            # print(f"img data: {aug_img is None}")
            if aug_img is not None:
                pil_img = Image.fromarray(aug_img)
                buf = io.BytesIO()
                pil_img.save(buf, format="PNG")
                buf.seek(0)
                img_name = str(uuid.uuid4()) + ".png"
                img_bytes = buf.getvalue()
                operator.write(img_name, img_bytes)

                out_base64 = base64.b64encode(img_bytes).decode("utf-8")
                point = cnn_measure(req.b64, out_base64)

                resp: CvAugmentResponse = CvAugmentResponse(
                    img_url=img_name, point=point
                )

                yield f"path: {resp.model_dump_json()}\n"

    return EventSourceResponse(
        image_generator(),
        media_type="text/event-stream",
    )


meta_prompt = """
ä½ æ˜¯ä¸“ä¸šçš„è§†è§‰æç¤ºè¯å·¥ç¨‹å¸ˆï¼Œæ“…é•¿ä¸ºStable Diffusionã€MidJourneyç­‰å›¾åƒç”Ÿæˆæ¨¡åž‹è®¾è®¡é«˜è´¨é‡æç¤ºè¯ã€‚

è¯·æ ¹æ®æˆ‘æä¾›çš„ç®€çŸ­ä¸­æ–‡æè¿°ï¼Œæ‰©å±•æˆä¸€æ¡è¯¦ç»†ã€ä¸°å¯Œã€è‹±æ–‡çš„Stable Diffusionå›¾åƒç”ŸæˆPromptï¼Œç”¨äºŽç”Ÿæˆé«˜æ¸…ã€é«˜è´¨é‡å›¾ç‰‡ã€‚

è¦æ±‚å¦‚ä¸‹ï¼š
	â€¢	å¿…é¡»ç”¨è‹±æ–‡è¾“å‡º
	â€¢	è¡¥å……ä¸°å¯Œçš„ç”»é¢ç»†èŠ‚ï¼ˆçŽ¯å¢ƒã€å…‰å½±ã€è‰²å½©ã€æ°›å›´ï¼‰
	â€¢	æŒ‡å®šè‰ºæœ¯é£Žæ ¼ï¼ˆå¦‚ï¼šçœŸå®žé£Žæ ¼ã€åŠ¨æ¼«é£Žã€æ²¹ç”»é£Žã€ç§‘å¹»é£Žã€æœªæ¥æ„Ÿã€å¤å¤é£Žã€3Dæ¸²æŸ“ç­‰ï¼Œè‹¥æè¿°æœªæåŠè¯·åˆç†è¡¥å……ï¼‰
	â€¢	æ·»åŠ è´¨é‡å…³é”®è¯ï¼ˆå¦‚ï¼šhighly detailedã€ultra realisticã€8Kã€masterpieceã€cinematic lightingã€high qualityï¼‰
	â€¢	æ•´ä½“é£Žæ ¼è‡ªç„¶æµç•…ï¼Œé€‚åˆç›´æŽ¥ç”¨äºŽStable Diffusion

ç¤ºä¾‹ï¼š

è¾“å…¥ï¼šä¸€åªæ¹–è¾¹çš„çŒ«
è¾“å‡ºï¼šA cute cat sitting by the calm lakeside during golden hour, warm sunset glow reflecting on rippling water, lush green trees in the background, ultra detailed fur, cinematic lighting, photorealistic style, 8K resolution, masterpiece, high quality

è¯·æ ¹æ®ä»¥ä¸Šè§„åˆ™ï¼Œæ‰©å±•ä»¥ä¸‹ä¸­æ–‡æè¿°ï¼š

{{prompt}}
"""


@router.post("/sd/prompt/optimize")
def optimize_prompt(req: PromptOptimizeRequest, db: Session = Depends(get_db)):
    """
    ä¼˜åŒ–prompt
    """
    tool_model = get_tool_model(db, req.model_id)
    model: OpenAI = get_model(tool_model)
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": meta_prompt.replace("{{prompt}}", req.prompt),
        },
    ]
    # response = model.chat(, model_name=tool_model.model_name)
    completion = model.chat.completions.create(
        model=tool_model.model_name,
        max_tokens=1024,
        messages=messages,
        temperature=0.7,
    )
    res = completion.choices[0].message.content
    return create_response(
        status=200, message="OK", data=PromptOptimizeResponse(prompt=res)
    )


@router.post("/measure/stream")
async def measure_stream(req: MeasureRequest, db: Session = Depends(get_db)):
    tool_model = get_tool_model(db, req.model_id)

    async def measure_generator():
        loop = asyncio.get_event_loop()
        # f = await loop.run_in_executor(
        #     None, functools.partial(cnn_measure, req.img1, req.img2)
        # )
        # yield f"score: {f}\n"
        await asyncio.sleep(0.5)
        s = await loop.run_in_executor(
            None, functools.partial(openai_measure, req.img1, req.img2, tool_model)
        )
        yield f"{s}\n"

    return EventSourceResponse(
        measure_generator(),
        media_type="text/event-stream",
    )


@router.get("/graph/embedded/{model_id}")
def get_graph(model_id: str):
    from pathlib import Path
    if model_id == "SimpleGan":
        graph = Path(__file__).parent / "resources/simple_gan.png"
        b64 = base64.b64encode(graph.read_bytes()).decode("utf-8")
        return {"data":f"data:image/png;base64,{b64}"}
    # TODO implement other models
    return {"data": ""}



@router.post("/train")
async def train(req: GANTrainRequest, db: Session = Depends(get_db)):
    pass


SD_CLIENT: Optional[StableDiffusionUnified] = None


@router.post("/sd/initialize")
async def initialize_sd(req: SDInitializeRequest):
    global SD_CLIENT
    try:
        enable_img2img = req.enable_img2img if req.enable_img2img is not None else False
        enable_inpaint = req.enable_inpaint if req.enable_inpaint is not None else False
        sd_path = req.model_path if req.model_path is not None else "/root/models/sd3m"
        SD_CLIENT = StableDiffusionUnified(sd_path, enable_img2img, enable_inpaint)
        return {"status": "ok"}
    except Exception as e:
        logger.error(e)
        return {"status": "error"}


@router.post("/sd/generate")
async def sd_augment(req: SdAugmentRequest, db: Session = Depends(get_db)):
    global SD_CLIENT
    if SD_CLIENT is None:
        logger.error("SD_CLIENT is not initialized")
        return {"status": "error", "message": "SD_CLIENT is not initialized"}
    operator = get_operator(s3_properties.augment_bucket_name)
    if req.job_type == "img2img":
        if req.img is None:
            return {"status": "error", "message": "img is required"}

        base64_img = req.img

        if req.prompt_optimize and req.model_id is not None:
            tool_model = get_tool_model(db, req.model_id)
            model: OpenAI = get_model(tool_model)
            logger.info(f"[tool model] {tool_model.id}.{tool_model.model_name} ")
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒåˆ†æžä¸Žæç¤ºè¯ç”ŸæˆåŠ©æ‰‹ï¼Œä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„å‚è€ƒå›¾åƒå†…å®¹ï¼Œåˆ†æžå…¶åœºæ™¯ã€é£Žæ ¼ã€å…ƒç´ ï¼Œç„¶åŽè‡ªåŠ¨ç”Ÿæˆç”¨äºŽ img2img ä»»åŠ¡çš„ prompt å’Œ negative promptã€‚",
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": (
                                "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒåˆ†æžåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹æ–¹å›¾åƒï¼Œåˆ†æžå…¶å†…å®¹ï¼Œå¹¶è¿”å›žé€‚ç”¨äºŽ Stable Diffusion img2img æ¨¡å¼çš„ prompt å’Œ negative_promptã€‚"
                                "\n\n"
                                "è¯·è¿”å›žå¦‚ä¸‹æ ¼å¼çš„ JSONï¼š\n"
                                "{\n"
                                '  "prompt": "...",\n'
                                '  "negative_prompt": "..."\n'
                                "}\n\n"
                                "è¦æ±‚ï¼š\n"
                                "- prompt ä¸ºè‹±æ–‡ï¼Œæè¿°å›¾åƒå†…å®¹çš„çœŸå®žæ„Ÿæž„å›¾ã€ç»†èŠ‚ã€é£Žæ ¼ä¿®é¥°è¯ã€‚\n"
                                "- negative_prompt åŒ…æ‹¬ä¸å¸Œæœ›å‡ºçŽ°çš„å†…å®¹ï¼Œä¾‹å¦‚æ¨¡ç³Šã€å¡é€šã€ä½Žè´¨é‡ã€ä½Žåˆ†è¾¨çŽ‡ã€åŠ¨æ¼«é£Žæ ¼ç­‰ã€‚\n"
                                "- é£Žæ ¼æ•´ä½“åçœŸå®žï¼Œä¸è¦å¤¸å¼ è‰ºæœ¯å¤„ç†ã€‚\n"
                                "- ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼Œä¸èƒ½è¾“å‡ºè‡ªç„¶è¯­è¨€è¯´æ˜Žã€‚\n"
                            ),
                        },
                        {
                            "type": "image_url",
                            "image_url": {"url": f"{base64_img}"},
                        },
                    ],
                },
            ]
            try:
                completion = model.chat.completions.create(
                    model=tool_model.model_name,
                    max_tokens=1024,
                    messages=messages,
                    temperature=0.7,
                )
                response = (
                    completion.choices[0]
                    .message.content.replace("```json", "")
                    .replace("```", "")
                    .strip()
                )
                logger.info(f"merge user`s prompt with llm generated prompt")
                prompt = json.loads(response)
                req.prompt = prompt["prompt"] + "," + req.prompt
                req.negative_prompt = prompt["negative_prompt"]
            except Exception:
                logger.error(
                    "prompt optimize failed because of llm or json parse error. Details:\n"
                )
                traceback.print_exc()

        async def __img_to_img():
            if req.negative_prompt is None:
                req.negative_prompt = __RESERVED_NEGATIVE_PROMPT__

            logger.info(f"prompt: {req.prompt}")
            logger.info(f"negative_prompt: {req.negative_prompt}")

            ref_img = base64_to_pil_image(base64_img)
            for i in range(req.count):
                res = SD_CLIENT.img2img(
                    image=ref_img,
                    prompt=req.prompt,
                    negative_prompt=req.negative_prompt,
                    steps=req.steps,
                    strength=req.strength,
                    guidance_scale=req.guidance_scale,
                    seed=random.randint(1, req.seed + i * 10),
                    num_images_per_prompt=1,
                )
                if res is None or len(res) == 0:
                    continue
                pil_img = res[0]
                buf = io.BytesIO()
                pil_img.save(buf, format="PNG")
                buf.seek(0)
                img_name = str(uuid.uuid4()) + ".png"
                img_bytes = buf.getvalue()
                operator.write(img_name, img_bytes)
                # yield f"path: {img_name}\n"
                ref_img_base64 = pil_to_base64(ref_img)
                out_base64 = base64.b64encode(img_bytes).decode("utf-8")
                point = cnn_measure(ref_img_base64, out_base64)

                resp: CvAugmentResponse = CvAugmentResponse(
                    img_url=img_name, point=point
                )

                yield f"path: {resp.model_dump_json()}\n"

                await asyncio.sleep(0.5)

        return EventSourceResponse(__img_to_img(), media_type="text/event-stream")

    if req.job_type == "inpaint":
        if req.img is None or req.mask is None:
            # TODO unimplemented yet
            return {"status": "error", "message": "img and mask are required"}
        return {"status": "error", "message": "inpaint is not supported yet"}

    if req.job_type == "txt2img":
        if req.lora_name is not None:
            if req.lora_name not in reserved_lora_modules:
                return {"status": "error", "message": "lora_name is not supported"}
            if req.lora_name in reserved_lora_modules:
                SD_CLIENT.enable_lora(req.lora_name)
                req.prompt = (
                    req.prompt
                    if len(req.prompt) == 0
                    else reserved_lora_modules[req.lora_name]["prompt"]
                )

        async def __txt_to_img():
            for i in range(req.count):
                res = SD_CLIENT.text2img(
                    prompt=req.prompt,
                    negative_prompt=req.negative_prompt,
                    width=req.width,
                    height=req.height,
                    steps=req.steps,
                    guidance_scale=req.guidance_scale,
                    seed=random.randint(1, req.seed + i * 10),
                    num_images_per_prompt=1,
                )
                if res is None or len(res) == 0:
                    continue
                pil_img = res[0]
                buf = io.BytesIO()
                pil_img.save(buf, format="PNG")
                buf.seek(0)
                img_name = str(uuid.uuid4()) + ".png"
                img_bytes = buf.getvalue()
                operator.write(img_name, img_bytes)
                # yield f"path: {img_name}\n"

                resp: CvAugmentResponse = CvAugmentResponse(img_url=img_name, point=0)

                yield f"path: {resp.model_dump_json()}\n"
                await asyncio.sleep(0.5)
            SD_CLIENT.disable_lora()

        return EventSourceResponse(__txt_to_img(), media_type="text/event-stream")

    return {"status": "error", "message": "Invalid request"}


@router.get("/on")
async def is_client_on():
    global SD_CLIENT
    return {"status": "ok", "data": SD_CLIENT is not None}


@router.post("/sd/deep-optimize")
async def sd_deep_optimize(req: SdDeepOptimizeRequest, db: Session = Depends(get_db)):
    tool_model = get_tool_model(db, req.model_id)
    model: OpenAI = get_model(tool_model)
    operator = get_operator(s3_properties.augment_bucket_name)
    first_image_path = req.img
    first_image_bytes = operator.read(first_image_path)
    first_image_base64 = base64.b64encode(first_image_bytes).decode("utf-8")
    global SD_CLIENT

    def __evaluate_image_with_mllm(base64_image: str, user_goal: str) -> dict:
        query = f"""
            ä½ æ˜¯ä¸€ä½å›¾åƒè¯„ä¼°ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ç”¨æˆ·ç›®æ ‡å¯¹å›¾åƒè¿›è¡Œè¯„ä¼°ï¼š

            ç›®æ ‡ï¼šã€Œ{user_goal}ã€

            è¯·ï¼š
            1. åˆ¤æ–­å›¾åƒæ˜¯å¦ç¬¦åˆè¯¥ç›®æ ‡ï¼ˆå¦‚çœŸå®žæ€§ã€å¯¹æ¯”åº¦ç­‰ï¼‰
            2. å¦‚æžœä¸ç¬¦åˆï¼ŒæŒ‡å‡ºä¸è¶³ï¼Œå¹¶ä¼˜åŒ–æç¤ºè¯ç”¨äºŽ Stable Diffusion ç”Ÿæˆ
            3. å›žç­”å¼ºåˆ¶ä½¿ç”¨è‹±è¯­

            è¯·ä¸¥æ ¼è¾“å‡ºä»¥ä¸‹ JSON æ ¼å¼ï¼ˆæ³¨æ„å­—æ®µåï¼‰ï¼š
            {{
                "score": 0~1ä¹‹é—´çš„æµ®ç‚¹æ•°,
                "advice": "ä½ å¯¹å›¾åƒä¼˜åŒ–çš„ç®€è¦å»ºè®®",
                "prompt": "æ–°çš„æ­£å‘æç¤ºè¯ï¼Œç”¨äºŽæŒ‡å¯¼ SD ç”Ÿæˆ",
                "negative_prompt": "æ–°çš„è´Ÿå‘æç¤ºè¯ï¼Œç”¨äºŽé¿å…ç”Ÿæˆé”™è¯¯"
            }}
            å¦‚æžœå›¾åƒå·²ç»å¾ˆå¥½ï¼Œä»ç„¶éœ€è¦è¾“å‡º score å’Œå»ºè®®ã€‚
        """

        if not base64_image.startswith("data:image/png;base64,"):
            base64_image = "data:image/png;base64," + base64_image

        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": query,
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"{base64_image}"},
                    },
                ],
            },
        ]

        completion = model.chat.completions.create(
            model=tool_model.model_name,
            max_tokens=1024,
            messages=messages,
            temperature=0.7,
        )
        response = (
            completion.choices[0]
            .message.content.replace("```json", "")
            .replace("```", "")
            .strip()
        )

        logger.info(f"ðŸ§  MLLM å“åº”ï¼š{response}")

        try:
            result = json.loads(response)

            if result.get("score", 0.0) >= 0.95:
                return {"status": "ok", **result}
            else:
                return {"status": "need_improve", **result}
        except Exception:
            logger.error(
                "prompt optimize failed because of llm or json parse error. Details:\n"
            )
            traceback.print_exc()
            return {
                "status": "error",
                "message": "Invalid response from Qwen-VL",
            }

    async def __loop_optimize(base64_image: str, user_goal: str, times: int):
        for i in range(times):
            res = __evaluate_image_with_mllm(base64_image, user_goal)
            if res["status"] == "ok":
                # save to s3
                img_name = str(uuid.uuid4()) + ".png"
                img_bytes = base64.b64decode(base64_image)
                operator.write(img_name, img_bytes)
                sdop: SdDeepOptimizeResponse = SdDeepOptimizeResponse(
                    img=img_name, tip=f"[Round {i+1}] done"
                )
                yield f"path: {sdop.model_dump_json()}\n"
                await asyncio.sleep(0.5)
                break
            elif res["status"] == "need_improve":
                prompt = res["prompt"]
                negative_prompt = res["negative_prompt"]
                image = base64_to_pil_image(base64_image)

                imgs = SD_CLIENT.img2img(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=image,
                    strength=0.3,
                )
                if imgs is None or len(imgs) == 0:
                    continue
                new_img = imgs[0]
                base64_image = pil_to_base64(new_img)
                img_name = str(uuid.uuid4()) + ".png"
                img_bytes = base64.b64decode(base64_image)
                operator.write(img_name, img_bytes)
                sdop: SdDeepOptimizeResponse = SdDeepOptimizeResponse(
                    img=img_name, tip=f"[Round {i+1}] {res['advice']}"
                )
                yield f"path: {sdop.model_dump_json()}\n"
                await asyncio.sleep(0.5)
            else:
                yield f"loop error \n"

    return EventSourceResponse(
        __loop_optimize(first_image_base64, req.prompt, req.loop_times),
        media_type="text/event-stream",
    )
